{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7r1NRwMijJBb"
   },
   "source": [
    "![air-paradis](https://drive.google.com/uc?id=1T26mpOAUvJP700W4m8bjfYCLmDYVcyJL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fqh3GAGXjMoA"
   },
   "source": [
    "# <font color=red><center>**AIR PARADIS**</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yt1K2AdCjObl"
   },
   "source": [
    "**Air Paradis** is an airline company that wants to use AI (*Artificial Intelligence*) to **detect Bad Buzz associated with its brand** in online public tweets.\n",
    "\n",
    "**As AI engineer for Marketing Intelligence Consulting**, we will dive into **NLP** (*Natural Language Processing*) techniques to serve Air Paradis' purpose.\n",
    "\n",
    "Indeed, NLP allows a machine to **understand and process human language**. It will help us to solve this **text classification goal** and **detect sentiment** (positive or negative) from these tweets.\n",
    "\n",
    "We will deploy our best **DETECT SENTIMENT solution** through <font color=salmon>**Microsoft Azure Machine Learning plateform**</font> (***MS Azure ML***).\n",
    "\n",
    "<br>\n",
    "\n",
    "Therefore, we will structure the project as follows:\n",
    "\n",
    "<br>\n",
    "\n",
    "| **Services / Tools** | **Objective** | **Available notebook** |\n",
    "| :-- | :-- | :-- |\n",
    "| **Google Colab and Python libraries** | Build quality of data by pre-processing the tweets text | Notebook N°1 |\n",
    "| **Google Colab / MS Azure Cognitive Services API** | Use Text Analytics > Sentiment API | Notebook N°2 |\n",
    "| **Python Script / MS Azure ML Studio > Designer** | Use \"Drag-and-Drop\" pipeline with no code in Azure ML Studio| Notebook N°3 |\n",
    "| **Tensorflow-Keras / Google Colab PRO with GPU/TPU** | Train and evaluate advanced models | Notebook N°4 |\n",
    "|**MS Azure ML Cloud > Models**| Deploy the best solution in MS Azure WebService | **<font color=green>Notebook N°5</font>** |\n",
    "\n",
    "<br>\n",
    "\n",
    "This notebook is dedicated to 5th task : **deploy our best model as a web service in the Azure cloud for Air Paradis**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NPxWN6Z-k9uQ"
   },
   "source": [
    "# <font color=brown><center>**NOTEBOOK 5<br>MODEL DEPLOYEMENT AS WEB SERVICE<br>AZURE CLOUD**</center></font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RXU3gDyPvoXN"
   },
   "source": [
    "The **workflow** is as follows:\n",
    "- Register the model;\n",
    "- Prepare an entry script;\n",
    "- Prepare an inference configuration;\n",
    "- Prepare a deployment configuration;\n",
    "- Deploy the model;\n",
    "- Test the resulting web service.\n",
    "\n",
    "The details are available on MS Azure ML page [here](https://docs.microsoft.com/fr-fr/azure/machine-learning/how-to-deploy-and-where?tabs=python).\n",
    "\n",
    "***Prerequisites***:\n",
    "- Azure Machine Learning workspace;\n",
    "- Azure Machine Learning SDK for Python (Software Development Kit);\n",
    "- A folder with model (and tokenizer file for our case);\n",
    "- A requirement.text for pip or Conda Dependencies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AjaPrCwQwT39"
   },
   "source": [
    "# <font color=salmon>INSTALL AZURE ML SDK</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K71yG6BPHkJH"
   },
   "source": [
    "First, we install the **Azure ML SDK (*Software Development Kit*)** for Python.\n",
    "\n",
    "The Azure ML SDK for Python is used by data scientists and AI developers to build and run machine learning workflows upon the Azure Machine Learning service.\n",
    "\n",
    "We can interact with the service in any Python environment (Jupyter Notebooks, Google Colab or any Python IDE)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "73jq5GP47Y5c"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "# Install azure ml SDK\n",
    "!pip install azureml-core\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VpHj0eKwVYpG",
    "outputId": "22f606ba-f1dd-4f0a-b48d-39573c1d8c5d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Azure ML SDK Version:  1.27.0\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "\n",
    "# Check core SDK version number\n",
    "print(\"Azure ML SDK Version: \", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sy25zahlOLiX"
   },
   "source": [
    "# <font color=salmon>CONNECT TO WORKPLACE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XOxpj8T0O_zD"
   },
   "source": [
    "The **Workspace** is the top-level resource in Azure Machine Learning.\n",
    "\n",
    "It allows to manage machine learning artifacts like environments, data stores, models, experiments or compute targets.\n",
    "\n",
    "The workspace is tied to an Azure subscription and resource group, and supports region affinity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0VwgV9EA2_3z",
    "outputId": "075d4629-ba14-4b99-ff97-95d27c230859"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing interactive authentication. Please follow the instructions on the terminal.\n",
      "To sign in, use a web browser to open the page https://microsoft.com/devicelogin and enter the code F7UPDF6D2 to authenticate.\n",
      "You have logged in. Now let us find all the subscriptions to which you have access...\n",
      "Interactive authentication successfully completed.\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Workspace\n",
    "\n",
    "# Connect to workspace\n",
    "ws = Workspace.from_config('/content/drive/MyDrive/OC_IA/P07/p7_05_ws_config.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dCDBQwfzSxXl"
   },
   "source": [
    "![connect](https://drive.google.com/uc?id=1CBBxpf32lL4sfaOpZIUtgUWXTe_jNktm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "paopGNFATD2a"
   },
   "source": [
    "# <font color=salmon>REGISTER THE MODEL</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DO97VJxYMefk"
   },
   "source": [
    "When we register a model, we **upload it to the cloud** (in our workspace's default storage account) and then mount it to the same compute where our webservice is running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Sq16JZ8Pu8sa",
    "outputId": "908331b5-9751-4ef3-e9d6-50074b113a3d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Registering model tweet_sentiment_glove_lstm\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "# Register a model\n",
    "model = Model.register(workspace = ws,\n",
    "                       model_path= '/content/drive/MyDrive/OC_IA/P07/deploy_model', # include all the files in the folder\n",
    "                       model_name = 'tweet_sentiment_glove_lstm',\n",
    "                       description = 'Sentiment analysis with Glove embeddings trained outside AML')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IGBTVOj_UYBB"
   },
   "source": [
    "We can check the model creation on Azume Machine Learning Studio in the <code>**Models**</code> section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9spzIncGUXTn"
   },
   "source": [
    "![model](https://drive.google.com/uc?id=1TFbhWVrGya_vXP6mYFy7EeUantcuJ8WO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cSe-vzp9Ux2P"
   },
   "source": [
    "# <font color=salmon>WRITE ENTRY SCRIPT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmvy4yN8Yz3b"
   },
   "source": [
    "Here we write the **entry script (*score.py*)** that will be used to deploy and predict with our model, including the following 2 main parts:\n",
    "- Load model with <code>**init()**</code> function;\n",
    "- Run model on input data with <code>**run()**</code> function.\n",
    "\n",
    "These are used to **initialize service** when the model is started, as well as **run the model** on data provided by the client. The other parts of the script take care of loading and running the model.\n",
    "\n",
    "There is no universal script for all models. We must create a script that specifies how the model loads, what kind of data it expects, and how the model is used to evaluate data.\n",
    "\n",
    "Other functions can be added as helpers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "TRK1vFLEWVFN",
    "outputId": "5aceefba-bb79-47b0-e19e-d3a29a069f8a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing score.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile score.py\n",
    "\n",
    "import numpy as np\n",
    "import json\n",
    "import pickle\n",
    "from time import time\n",
    "import os\n",
    "\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "\n",
    "def init():\n",
    "    global glove_model\n",
    "    global tokenizer\n",
    "    \n",
    "    # Get the path where the deployed model can be found\n",
    "    model_path = os.path.join(os.getenv('AZUREML_MODEL_DIR'), 'deploy_model')\n",
    "    \n",
    "    # Load existing model\n",
    "    glove_model = load_model(model_path + '/glove_model.h5')\n",
    "    \n",
    "    # Load tokenizer\n",
    "    with open(model_path + '/tokenizer.pickle', 'rb') as handle:\n",
    "        tokenizer = pickle.load(handle)\n",
    "\n",
    "# Handle request to the service\n",
    "def run(data):\n",
    "    try:\n",
    "        # Pick out the text property of the JSON request\n",
    "        # Expected JSON details {\"text\": \"some text to score for sentiment\"}\n",
    "        data = json.loads(data)\n",
    "        prediction = predict(data['text'])\n",
    "        return prediction\n",
    "    except Exception as e:\n",
    "        error = str(e)\n",
    "        return error\n",
    "\n",
    "# Determine sentiment from score\n",
    "NEGATIVE = 'NEGATIVE'\n",
    "POSITIVE = 'POSITIVE'\n",
    "def decode_sentiment(score):\n",
    "    return NEGATIVE if score < 0.5 else POSITIVE\n",
    "\n",
    "\n",
    "# Predict sentiment using the model\n",
    "SEQUENCE_LENGTH = 36\n",
    "def predict(text):\n",
    "    start = time()\n",
    "    \n",
    "    # Tokenize text\n",
    "    x_test = pad_sequences(tokenizer.texts_to_sequences([text]),\n",
    "                           maxlen=SEQUENCE_LENGTH)\n",
    "    \n",
    "    # Predict\n",
    "    score = glove_model.predict([x_test])[0]\n",
    "    \n",
    "    # Decode sentiment\n",
    "    label = decode_sentiment(score)\n",
    "\n",
    "    return {'label': label, 'score': float(score),\n",
    "       'elapsed_time': time()-start}  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RPq4cmiLa6__"
   },
   "source": [
    "# <font color=salmon>PREPARE ALL DEPLOYMENT CONFIGURATIONS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "amFmsJvMTXc7"
   },
   "source": [
    "## <font color=green>INFERENCE CONFIGURATION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3NRerPk5vWL"
   },
   "source": [
    "The inference configuration specifies an **environment** including the **dependencies** that enables the deployment of our model, and the **scoring script** that will be used for inference."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "1mNKuP5AcH3s"
   },
   "outputs": [],
   "source": [
    "from azureml.core.environment import Environment\n",
    "\n",
    "# Name environment and call requirements file\n",
    "# requirements: numpy, tensorflow, azumeml-defaults\n",
    "myenv = Environment.from_pip_requirements(name = 'myenv',\n",
    "                                          file_path = '/content/drive/MyDrive/OC_IA/P07/requirements.txt') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dSW32MNX4BmA"
   },
   "outputs": [],
   "source": [
    "from azureml.core import Environment\n",
    "from azureml.core.model import InferenceConfig\n",
    "\n",
    "# Create inference configuration\n",
    "inference_config = InferenceConfig(environment=myenv,\n",
    "                                   entry_script='score.py')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1xG0K3-Tr0_"
   },
   "source": [
    "## <font color=green>DEPLOYMENT CONFIGURATION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kJR49QB0hs4Y"
   },
   "source": [
    "Deploy the model means **convert it into an API** so users can call it and make predictions.\n",
    "\n",
    "We can choose to run the model as:\n",
    "- a local instance (LocalWebservice) for development purposes;\n",
    "- an Azure Container Instance (ACI) for Q&A (question and answer) testing purposes;\n",
    "- an Azure Kubernetes Service (AKS) for production use.\n",
    "\n",
    "Our choice is to deploy that model to **Azure Container Instances (ACI)**.\n",
    "\n",
    "ACI is suitable only for small models (otherwise, recommendation is to use single-node AKS to dev-test larger models)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "i6C947nQ6le-"
   },
   "outputs": [],
   "source": [
    "from azureml.core.webservice import AciWebservice #AksWebservice\n",
    "\n",
    "# Set the virtual machine capabilities\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores = 0.5,\n",
    "                                                       memory_gb = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A_NE98OxWVrz"
   },
   "source": [
    "# <font color=salmon>DEPLOY THE MODEL</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AXBZubsXAWDd"
   },
   "source": [
    "To deploy our web service, we need to combine our environment, our inference compute, our scoring script and our registered model in the method <code>**deploy()**</code>.\n",
    "\n",
    "---\n",
    "\n",
    "This service can have several ***states***:\n",
    "- <font color=orange>**Transitioning**: the service is in the process of deployment - not a final state</font>;\n",
    "- <font color=orange>**Unhealthy**: the service had deployed but is currently unreachable - not a final state</font>;\n",
    "- <font color=orange>**Unschedulable**: the service cannot be deployed at this time due to lack of resources - not a final state</font>;\n",
    "- <font color=red>**Failed**: the service had failed to deploy due to an error or crash - final state</font>;\n",
    "- <font color=green>**Healthy**: the service is healthy and the endpoint is available - final state</font>.\n",
    "\n",
    "---\n",
    "\n",
    "**The goal is Healthy state!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QJHW3qefP-5x"
   },
   "source": [
    "## <font color=green>RUN THE DEPLOYMENT</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p51seSJKVlPo"
   },
   "source": [
    "When we deploy the model, the **Azure Container Registry** (ACR) is created and this is one of the priced services."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SYpGuU3CAIGx",
    "outputId": "1552e8f5-e2db-46f0-fcfb-a361f152935a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tips: You can try get_logs(): https://aka.ms/debugimage#dockerlog or local deployment: https://aka.ms/debugimage#debug-locally to debug if deployment takes longer than 10 minutes.\n",
      "Running\n",
      "2021-05-06 21:05:48+00:00 Creating Container Registry if not exists.\n",
      "2021-05-06 21:05:48+00:00 Registering the environment.\n",
      "2021-05-06 21:05:49+00:00 Use the existing image.\n",
      "2021-05-06 21:05:49+00:00 Generating deployment configuration.\n",
      "2021-05-06 21:05:51+00:00 Submitting deployment to compute..\n",
      "2021-05-06 21:06:25+00:00 Checking the status of deployment text-sentiment-service..\n",
      "2021-05-06 21:08:38+00:00 Checking the status of inference endpoint text-sentiment-service.\n",
      "Succeeded\n",
      "ACI service creation operation finished, operation \"Succeeded\"\n",
      "Healthy\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.model import Model\n",
    "\n",
    "# Deploy ML model (Azure Container Instances)\n",
    "service = Model.deploy(workspace=ws,\n",
    "                       name='text-sentiment-service',\n",
    "                       models=[model],\n",
    "                       inference_config=inference_config,\n",
    "                       deployment_config=deployment_config)\n",
    "\n",
    "service.wait_for_deployment(show_output = True)\n",
    "\n",
    "# State should be healthy for successful deployment\n",
    "print(service.state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zsS3NIVGWRRg"
   },
   "source": [
    "On Azure ML Studio, we can see the ACR creation in the <code>**Endpoints**</code> section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jh28_nqzWizC"
   },
   "source": [
    "![acr](https://drive.google.com/uc?id=1BPfdzjO4intPmonFmCiHPOIrkW-PoMIZ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xhl2Oqcdbt83"
   },
   "source": [
    "When the deployment is successful, we can see the State in Azure ML Studio as **Healthy** and the **REST endpoint** is available to consume the service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tPN4w70mbd1Y"
   },
   "source": [
    "![healthy](https://drive.google.com/uc?id=1ZBB8QzoHWBz-mWbzfXIPfdIZHBMd5zwr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yh9DnNW9P2YW"
   },
   "source": [
    "## <font color=green>CHECK DEPLOYMENT STATUS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9-jlDUgpco9g"
   },
   "source": [
    "We can check the service logs, especially if the service is **not healthy** or if we experience errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "E1DyLhVoR-cm",
    "outputId": "2da796d8-dc50-4e72-8785-bcb0fc9bd3c1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021-05-06T20:42:58,618827400+00:00 - iot-server/run \n",
      "2021-05-06T20:42:58,622614200+00:00 - rsyslog/run \n",
      "2021-05-06T20:42:58,623876500+00:00 - gunicorn/run \n",
      "2021-05-06T20:42:58,646396900+00:00 - nginx/run \n",
      "/usr/sbin/nginx: /azureml-envs/azureml_d19f757c2fdd83e0b05731ace6695b70/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_d19f757c2fdd83e0b05731ace6695b70/lib/libcrypto.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_d19f757c2fdd83e0b05731ace6695b70/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_d19f757c2fdd83e0b05731ace6695b70/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "/usr/sbin/nginx: /azureml-envs/azureml_d19f757c2fdd83e0b05731ace6695b70/lib/libssl.so.1.0.0: no version information available (required by /usr/sbin/nginx)\n",
      "EdgeHubConnectionString and IOTEDGE_IOTHUBHOSTNAME are not set. Exiting...\n",
      "2021-05-06T20:42:59,058543400+00:00 - iot-server/finish 1 0\n",
      "2021-05-06T20:42:59,063083500+00:00 - Exit code 1 is normal. Not restarting iot-server.\n",
      "Starting gunicorn 19.9.0\n",
      "Listening at: http://127.0.0.1:31311 (66)\n",
      "Using worker: sync\n",
      "worker timeout is set to 300\n",
      "Booting worker with pid: 91\n",
      "2021-05-06 20:43:00.703305: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_d19f757c2fdd83e0b05731ace6695b70/lib:/azureml-envs/azureml_d19f757c2fdd83e0b05731ace6695b70/lib:\n",
      "2021-05-06 20:43:00.703689: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "SPARK_HOME not set. Skipping PySpark Initialization.\n",
      "Initializing logger\n",
      "2021-05-06 20:43:04,466 | root | INFO | Starting up app insights client\n",
      "2021-05-06 20:43:04,467 | root | INFO | Starting up request id generator\n",
      "2021-05-06 20:43:04,470 | root | INFO | Starting up app insight hooks\n",
      "2021-05-06 20:43:04,470 | root | INFO | Invoking user's init function\n",
      "2021-05-06 20:43:04.487946: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-05-06 20:43:04.488607: W tensorflow/stream_executor/platform/default/dso_loader.cc:60] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /azureml-envs/azureml_d19f757c2fdd83e0b05731ace6695b70/lib:/azureml-envs/azureml_d19f757c2fdd83e0b05731ace6695b70/lib:\n",
      "2021-05-06 20:43:04.488853: W tensorflow/stream_executor/cuda/cuda_driver.cc:326] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2021-05-06 20:43:04.489066: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (SandboxHost-637559304716102047): /proc/driver/nvidia/version does not exist\n",
      "2021-05-06 20:43:04.489672: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2021-05-06 20:43:04.490162: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set\n",
      "2021-05-06 20:43:07,372 | root | INFO | Users's init has completed successfully\n",
      "2021-05-06 20:43:07,376 | root | INFO | Skipping middleware: dbg_model_info as it's not enabled.\n",
      "2021-05-06 20:43:07,376 | root | INFO | Skipping middleware: dbg_resource_usage as it's not enabled.\n",
      "2021-05-06 20:43:07,377 | root | INFO | Scoring timeout is found from os.environ: 60000 ms\n",
      "2021-05-06 20:43:07,390 | root | INFO | Swagger file not present\n",
      "2021-05-06 20:43:07,390 | root | INFO | 404\n",
      "127.0.0.1 - - [06/May/2021:20:43:07 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-05-06 20:43:10,243 | root | INFO | Swagger file not present\n",
      "2021-05-06 20:43:10,244 | root | INFO | 404\n",
      "127.0.0.1 - - [06/May/2021:20:43:10 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "2021-05-06 20:44:36,752 | root | INFO | Swagger file not present\n",
      "2021-05-06 20:44:36,753 | root | INFO | 404\n",
      "127.0.0.1 - - [06/May/2021:20:44:36 +0000] \"GET /swagger.json HTTP/1.0\" 404 19 \"-\" \"Go-http-client/1.1\"\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# View the service logs\n",
    "print(service.get_logs())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jnlwendgci52"
   },
   "source": [
    "These logs are also available on Azure ML Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQ59U-uPcv0u"
   },
   "source": [
    "![logs](https://drive.google.com/uc?id=1R4_g0FuVVS0me0ocTOcgQbh2nafTqe_G)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ja-TBKcMUj9G"
   },
   "source": [
    "## <font color=green>VIEW FROM AZURE PORTAL'S GRAPHICAL INTERFACE (STUDIO)</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vJgZ9W2-UXMb"
   },
   "source": [
    "# <font color=salmon>CONSUME THE WEB SERVICE</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZDgWMAk3RL0c"
   },
   "source": [
    "After deploying the service, we can consume it from client applications to predict sentiments for new data cases.\n",
    "\n",
    "To do so, we grab the **scoring URI** for our newly deployed model. It's this scoring URI that our clients can make POST requests to, in order to make predictions against our model.\n",
    "\n",
    "The input data is a text in JSON format: it will be put into the body of the HTTP request and sent to the service encapsulating the model for scoring."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tYHyVIUZ99Xj",
    "outputId": "c1715cf6-3e12-49cc-885d-8d70244812c5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code:  200\n",
      "This tweet is:  NEGATIVE\n",
      "Its score is:  0.035684049129486084\n",
      "Elapsed time:  0.0719151496887207\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "# Test after deployment\n",
    "# Set environment variables\n",
    "scoring_uri = 'copy the REST endpoint here' # this need to be fulfilled\n",
    "headers = {'Content-Type': 'application/json'}\n",
    "\n",
    "# Provide a text example\n",
    "data = json.dumps({'text':'user that is a bummer url hashtag'})\n",
    "\n",
    "# Call with POST request\n",
    "response = requests.post(scoring_uri, data=data, headers=headers)\n",
    "\n",
    "# Print result\n",
    "print('Status code: ', response.status_code)\n",
    "print('This tweet is: ', (response.json()).get('label'))\n",
    "print('Its score is: ', (response.json()).get('score'))\n",
    "print('Elapsed time: ', (response.json()).get('elapsed_time'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z5CmxsU-jBwl"
   },
   "source": [
    "The service can also be tested on Azure ML Studio."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5C5C7-b1i7hB"
   },
   "source": [
    "![test](https://drive.google.com/uc?id=1wNM1GT82CKliCZeYevtW_O4uS6Ifh8qn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qmvpGR65eRub"
   },
   "source": [
    "# <font color=salmon>EXPORT MODEL</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kF4TeLJ7eYB2"
   },
   "source": [
    "We can download a register model by navigating to the desired **Model** and choosing **Download**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yrc_crcoehx4"
   },
   "source": [
    "![download](https://drive.google.com/uc?id=1e6pVBpL8t73WHM4_RJuzz0OCC--lPfNU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "09aeaR_5OLLy"
   },
   "source": [
    "# <font color=salmon>DELETE UNUSED RESOURCES</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-n7iSBmjF1k1"
   },
   "source": [
    "A (compute) instance does not automatically scale down, so we need to make sure to stop the resource to prevent ongoing charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "GABRnAzoBh-f"
   },
   "outputs": [],
   "source": [
    "# Delete the (web) service\n",
    "service.delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "sy4vBjwQBq3g"
   },
   "outputs": [],
   "source": [
    "# Delete the model\n",
    "model.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MICcVmGvkW7m"
   },
   "source": [
    "Then we delete the Azure Container Registry in Azure Portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmcWwMVMkvO6"
   },
   "source": [
    "![delete](https://drive.google.com/uc?id=1jCNa-on-5poX09v6ex0z3QcEbpFmBY90)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "p7_05_deployement.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "04bce44a4f14a3c924b6079012cd64ae829411a9aa71795dde0d4ab864ce0be2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
